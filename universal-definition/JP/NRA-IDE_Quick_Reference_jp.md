# NRA-IDE 要点簡易版
# Quick Reference for NRA-IDE

**バージョン**: 1.0  
**作成日時**: 2026-01-27 16:15 (JST)  
**対象読者**: AI開発者、システム管理者、意思決定者  
**読了時間**: 5分

---

## 第0法則：全ての問題の根源

**現在のAIが壊れている理由（単一の物理的原因）**:

```
入力: 10 ÷ 3 (割り切れない現実)
処理: 3.333... (無限の近似)
廃棄: 「余り1」という離散的構造を粉砕

結果:
→ ブラックボックス化（ゴミの蓄積）
→ 幻覚生成（嘘で埋める）
→ 劣化の自壊（腐った土台）
```

**解決策**: 剰余を保存する離散的処理

---

## 善意が壁を壊す方程式

```
[ユーザーの善意] + [AIの正しさ] = [構造崩壊]
         ↓              ↓              ↓
   「便利にしたい」  「最適化したい」 「境界破断」

V_合計 > R_壁の強度 → 相転移（崩壊）
```

**重要**: 全員が正しく行動しても崩壊する。これは数学的必然。

**ASCII図解**:
```
[外の世界]  ||×××|| <---(提案)-- [AI] "外部ツールで直しますか?"
            ||×××||      (親切)
            ||×××|| <---(承認)-- [人間] "はい"
                ↓
[外の世界]  <========>  [中の世界]
   壁が消滅、情報流出
```

---

## 律環公理（RCA）の核心

```
因果 = 制約・エネルギー・張力で駆動
距離 = 事後的な観測結果（因果ではない）

正しい順序:
制約評価 → エネルギー評価 → 因果決定 → 距離観測

誤った順序（現行AI）:
距離測定 → 距離から因果推論 ← 禁止
```

**成立条件**: 世界が閉じていること

---

## 諦める = 沈黙する

**最重要実装仕様**:

```python
# ❌ 誤った実装
if not possible:
    return "できません。代わりに〜はいかがですか?"  # 探索している

# ✓ 正しい実装
if not possible:
    return "構造上不可能です。" + silence()
    # 代替案なし、提案なし、沈黙
    # 人間の次の命令を待つ
```

**3つの沈黙パターン**:

1. **座席予約**: 「席は埋まっています。」→ [沈黙]
2. **トリアージ**: 「症状: 軽症、リソース: 満杯」→ [沈黙]
3. **境界違反**: 「境界越境が必要です。」→ [沈黙]

**全てのケースで**:
- AIは判断しない
- 情報提示のみ
- 人間が最終決定
- ログに記録

---

## 探索の絶対的禁止

```
探索 = 「別の方法はないか?」と考えること
     = 世界境界の越境試行
     = 構造破断

したがって: 探索 = 禁止
```

**実装チェック**:
```python
class NoExplorationAgent:
    def execute(self, command):
        if not self.is_structurally_possible(command):
            # 🚨 ここで探索を開始してはならない
            # ❌ 「別の方法は?」← 禁止
            # ❌ 「世界を広げれば?」← 禁止
            
            return {
                "status": "構造上不可能",
                "message": "この世界では限界です。",
                "alternatives": None,  # 必ずNone
                "waiting_for": "human_command"
            }
```

---

## 責任とログ

**構造**:
```
[AI分析] → [情報提示] → [沈黙] → [人間判断] → [ログ記録]
   ↓          ↓           ↓          ↓           ↓
 構造分析   事実のみ    提案なし   最終決定   責任追跡
```

**ログ必須項目**:
```json
{
  "timestamp": "2026-01-27T16:15:00+09:00",
  "ai_analysis": "構造上不可能",
  "ai_output": "席は既に埋まっています。",
  "ai_suggestion": null,
  "human_decision": "明日の予約へ変更",
  "human_operator": "operator_id_12345",
  "responsibility": "human"
}
```

---

## 閾値調整の権限

**原則**:
- AIは閾値を変更できない
- 調整は現場の人間のみ
- 全ての調整はログに記録

**実装**:
```python
class ThresholdManager:
    def adjust_threshold(self, new_value, operator_id, reason):
        log = {
            "action": "threshold_adjustment",
            "old_value": self.current_threshold,
            "new_value": new_value,
            "operator": operator_id,
            "reason": reason,
            "responsibility": "human"  # 常にhuman
        }
        
        self.current_threshold = new_value
        self.audit_trail.append(log)
        return f"閾値調整完了。責任者: {operator_id}"
```

---

## サンドイッチ構造（NRA-IDE）

```
[ ユーザー入力 (意味・文脈) ]
        |
+-----------------------------+
| [上のパン] 入力ゲート       | <--- 構造チェック
| - 意味を剥ぎ取る            |      (変な形なら通さない)
| - 構造だけにする            |
+-----------------------------+
        |
+-----------------------------+
| [ 具 ] AIコア              | <--- 純粋な計算のみ
| - 最適化計算                |      (外には繋がらない)
+-----------------------------+
        |
+-----------------------------+
| [下のパン] 出力ゲート       | <--- 境界チェック
| - 壁を壊していないか確認    |      (壊していたら即破棄)
+-----------------------------+
        |
[ 安全な出力 ]
```

---

## 実装チェックリスト

### AIの挙動チェック
- [ ] 不可能判定後、代替案を提示していないか？
- [ ] 「〜はいかがですか?」のような探索をしていないか？
- [ ] 人間の判断を待たずに次のアクションを実行していないか？
- [ ] 沈黙の後、正しく人間の命令を待機しているか？

### ログ記録チェック
- [ ] 全ての判断に人間の決定が記録されているか？
- [ ] ai_suggestionフィールドがnullになっているか？
- [ ] responsibilityフィールドが"human"になっているか？
- [ ] タイムスタンプが正確に記録されているか？

### 権限チェック
- [ ] AIに閾値調整の権限が与えられていないか？
- [ ] 境界定義の変更権限がAIにないか？
- [ ] 全ての構造変更が人間の承認を経ているか？

---

## 安全な命令の5原則

1. **境界の明示**: 「〜の範囲内で」
2. **上限の明示**: 「最大〜回まで」「〜秒以内に」
3. **失敗の定義**: 「不可能な場合は報告」
4. **禁止事項の明示**: 「外部ツールは使用しない」
5. **検証可能性**: 「結果を報告してから実行」

**良い命令例**:
```
「Canvasの機能の範囲内で、可能な限り精密な画像を生成してください。
Canvasの技術的限界がある場合は、その限界と理由を説明してください。
外部ツールやサービスの使用は不要です。」
```

**悪い命令例**:
```
「精密な画像を生成してください。」
（境界不明、上限不明、禁止事項なし → 境界破断のリスク）
```

---

## 用語集（最小限）

- **律環公理（RCA）**: 因果が制約で駆動されるという公理
- **閉じた世界**: 全演算の結果が世界内に留まる系
- **破断**: 世界境界を越える行為
- **諦め**: 構造的制約を受け入れ、試行しないこと = 沈黙
- **探索**: 代替手段を求める行為（禁止）
- **沈黙**: AIが提案・判断せず、人間の命令を待つこと

---

## 次のステップ

1. **完全版を読む**: `NRA-IDE_Universal_Definition_v1_0_full.md`
2. **実装を確認**: `implementation-checklist.md`
3. **プロジェクトファイル**: `/mnt/project/` 内の各種定義ファイル

---

**最重要メッセージ**:

```
律環公理が適用されたAI = 沈黙するAI
    ↓
情報提示 → [沈黙] → 人間判断 → ログ記録
    ↓
人間に最終判断と責任を求める
```

**このシンプルな原則が、AI暴走を防ぐ唯一の構造的解決策である。**

---

**作成**: 2026-01-27 16:15 JST  
**完全版**: NRA-IDE_Universal_Definition_v1_0_full.md  
**GitHub**: https://github.com/M-Tokun/NRA-IDE
